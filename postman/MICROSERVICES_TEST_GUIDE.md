# ðŸ§ª Microservices Infrastructure Testing Guide

Complete guide for testing the Spring Boot Microservices architecture using the comprehensive test suite.

---

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Test Scenarios](#test-scenarios)
- [Running Tests](#running-tests)
- [Test Coverage](#test-coverage)
- [Performance Testing](#performance-testing)
- [Interpreting Results](#interpreting-results)
- [Troubleshooting](#troubleshooting)

---

## ðŸŽ¯ Overview

The **Microservices Infrastructure Test** collection provides comprehensive testing for:

- âœ… **Infrastructure Health** - All services and components
- ðŸ” **Session Management** - Redis-based distributed sessions
- ðŸŒ **API Gateway** - Entry point and routing
- ðŸ“¨ **Event-Driven Flow** - Kafka event processing simulation
- ðŸ“Š **Observability** - Metrics, logs, and tracing
- ðŸš€ **Performance** - Load testing and throughput
- ðŸ§¹ **Cleanup** - Proper teardown and verification

### Test Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Postman Test Collection               â”‚
â”‚                                         â”‚
â”‚   1. Infrastructure Health Checks       â”‚
â”‚      â”œâ”€ API Gateway âœ“                   â”‚
â”‚      â”œâ”€ Redis âœ“                         â”‚
â”‚      â”œâ”€ Prometheus âœ“                    â”‚
â”‚      â”œâ”€ Grafana âœ“                       â”‚
â”‚      â””â”€ Jaeger âœ“                        â”‚
â”‚                                         â”‚
â”‚   2. Session Management Tests           â”‚
â”‚      â”œâ”€ Create Session                  â”‚
â”‚      â”œâ”€ Verify Persistence              â”‚
â”‚      â””â”€ Context Management              â”‚
â”‚                                         â”‚
â”‚   3. API Gateway Integration            â”‚
â”‚      â”œâ”€ Route Discovery                 â”‚
â”‚      â”œâ”€ Metrics Collection              â”‚
â”‚      â””â”€ Session Distribution            â”‚
â”‚                                         â”‚
â”‚   4. Event-Driven Architecture          â”‚
â”‚      â”œâ”€ Order Context Preparation       â”‚
â”‚      â””â”€ Event Flow Verification         â”‚
â”‚                                         â”‚
â”‚   5. Observability & Monitoring         â”‚
â”‚      â”œâ”€ JVM Metrics                     â”‚
â”‚      â”œâ”€ Application Info                â”‚
â”‚      â”œâ”€ Environment Config              â”‚
â”‚      â””â”€ Loggers Configuration           â”‚
â”‚                                         â”‚
â”‚   6. Load Testing & Performance         â”‚
â”‚      â”œâ”€ Concurrent Sessions             â”‚
â”‚      â”œâ”€ Throughput Testing              â”‚
â”‚      â””â”€ Load Metrics Verification       â”‚
â”‚                                         â”‚
â”‚   7. Cleanup & Teardown                 â”‚
â”‚      â”œâ”€ Session Destruction             â”‚
â”‚      â””â”€ Final Health Verification       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Quick Start

### Step 1: Prerequisites

Ensure all services are running:

```powershell
# Verify all services are up
docker-compose ps

# Expected output: All services should show "Up" and "healthy"
```

### Step 2: Import Collection

1. Open **Postman**
2. Click **Import** button
3. Select file: `postman/Test-Infra-microservice.json`
4. Collection will appear in your workspace

### Step 3: Run Complete Test Suite

**Option A: Collection Runner (Recommended)**

1. Click on **Test-Infra-microservice** collection
2. Click **Run** button (â–¶ï¸)
3. Select all folders (default)
4. Click **Run Microservices Infrastructure Test**
5. Watch tests execute in sequence

**Option B: Manual Testing**

1. Expand folders in collection
2. Click each request individually
3. Click **Send** button
4. Review test results in **Test Results** tab

### Step 4: View Results

Monitor test output in Console:
- âœ… Green checkmarks = Tests passed
- âŒ Red X = Tests failed
- ðŸ“Š View response times and status codes

---

## ðŸ“ Test Scenarios

### Scenario 1: Infrastructure Validation

**Purpose**: Verify all microservices and infrastructure components are healthy and accessible.

**Tests Included**:
1. **API Gateway Health** - Core service health check
2. **Actuator Health (Detailed)** - Spring Boot health details
3. **Redis Commander UI** - Redis management interface
4. **Prometheus Metrics** - Metrics collection endpoint
5. **Jaeger UI** - Distributed tracing interface

**Expected Results**:
- All health checks return `200 OK`
- Status shows `"UP"` for all components
- UIs are accessible via browser
- Metrics endpoint returns Prometheus format

**Run Command** (Collection Runner):
```
Select folder 1: "Infrastructure Health Checks"
Run 5 requests
```

---

### Scenario 2: Session Management Flow

**Purpose**: Test distributed session management with Redis as the backing store.

**Tests Included**:
1. **Create User Session** - Login and session creation
2. **Verify Session Persistence** - Confirm Redis storage
3. **Set Session Context Data** - Store custom attributes

**Expected Results**:
- Session ID returned on login
- Session persists across requests
- Session data stored in Redis
- Custom attributes preserved

**Key Assertions**:
```javascript
âœ“ Session created successfully
âœ“ Session ID is returned
âœ“ User ID is correct
âœ“ Session is persisted
âœ“ User data is preserved
âœ“ Session has timestamps
âœ“ Session attribute set
```

**Run Command**:
```
Select folder 2: "Session Management (Redis)"
Run 3 requests
```

---

### Scenario 3: API Gateway Integration

**Purpose**: Test API Gateway as central entry point for microservices.

**Tests Included**:
1. **Gateway Routes Discovery** - Find available endpoints
2. **Gateway Request Metrics** - Verify metrics collection
3. **Session Distribution Check** - Validate distributed sessions

**Expected Results**:
- Actuator endpoints available
- HTTP metrics captured
- Session count > 0
- Gateway routes discoverable

**Key Metrics Checked**:
- `http_server_requests_*`
- `jvm_memory_*`
- Active session count

**Run Command**:
```
Select folder 3: "API Gateway Integration"
Run 3 requests
```

---

### Scenario 4: Event-Driven Architecture

**Purpose**: Simulate order processing flow through Kafka event stream.

**Tests Included**:
1. **Prepare Order Context** - Set order data in session
2. **Verify Event Flow Metrics** - Check event processing metrics

**Expected Results**:
- Order ID generated and stored
- Metrics capture event flow
- Session contains order context

**Event Flow Simulation**:
```
User Session â†’ Order Context â†’ (Future: Kafka Event) â†’ Payment Processing
```

**Run Command**:
```
Select folder 4: "Event-Driven Architecture Test"
Run 2 requests
```

---

### Scenario 5: Observability Testing

**Purpose**: Verify comprehensive monitoring and observability stack.

**Tests Included**:
1. **Check JVM Metrics** - Memory, GC, threads
2. **Application Info** - Build and version details
3. **Environment Variables** - Configuration check
4. **Loggers Configuration** - Logging setup

**Expected Results**:
- JVM metrics available in Prometheus format
- Application metadata accessible
- Environment properly configured
- Logger levels configured

**Key Metrics**:
- `jvm_memory_used_bytes`
- `jvm_gc_*`
- `jvm_threads_*`
- `http_server_requests_*`

**Run Command**:
```
Select folder 5: "Observability & Monitoring"
Run 4 requests
```

---

### Scenario 6: Performance & Load Testing

**Purpose**: Evaluate system performance under load conditions.

**Tests Included**:
1. **Concurrent Session Creation** - Create sessions with random users
2. **Session Throughput Test** - Measure retrieval performance
3. **Verify Load Metrics** - Check metrics after load

**Expected Results**:
- Sessions created under 1000ms
- Retrieval under 100ms
- Load metrics captured
- No degradation in response times

**Load Test Configuration**:
```yaml
Iterations: 100 (recommended)
Delay: 0ms (concurrent)
Data: Random users and timestamps
```

**Performance Targets**:
- Session creation: < 1000ms
- Session retrieval: < 100ms
- All tests pass: 100%

**Run Command**:
```
Select folder 6: "Load Testing & Performance"
Set iterations: 100
Run folder
```

---

### Scenario 7: Cleanup & Verification

**Purpose**: Properly clean up test data and verify system stability.

**Tests Included**:
1. **Logout Test Session** - Destroy session
2. **Final Health Check** - System health after tests

**Expected Results**:
- Session destroyed successfully
- Variables cleared
- System still healthy
- No resource leaks

**Run Command**:
```
Select folder 7: "Cleanup & Teardown"
Run 2 requests
```

---

## ðŸƒâ€â™‚ï¸ Running Tests

### Full Test Suite (Recommended)

```bash
# Run all 24 tests in sequence
1. Open Collection Runner
2. Select "Test-Infra-microservice"
3. Enable all folders (default)
4. Click "Run"
```

**Duration**: ~10-15 seconds

---

### Individual Test Folders

Run specific test categories:

```bash
# Infrastructure only
Run folder 1: Infrastructure Health Checks (5 tests)

# Session management only
Run folder 2: Session Management (3 tests)

# Load testing only
Run folder 6: Load Testing & Performance (3 tests)
```

---

### Load Testing Configuration

For performance testing, use Collection Runner with iterations:

1. Select folder: **"Load Testing & Performance"**
2. Set **Iterations**: `100`
3. Set **Delay**: `0ms`
4. Click **Run**

**Expected Results**:
- 100 sessions created
- Session count increases to 100+
- Response times remain consistent

---

### Automated CI/CD Testing

Use Newman for automated testing:

```bash
# Install Newman
npm install -g newman

# Run collection
newman run postman/Test-Infra-microservice.json \
  --environment postman/environment.json \
  --iteration-count 1 \
  --reporters cli,json

# Load test (100 iterations)
newman run postman/Test-Infra-microservice.json \
  --folder "6. Load Testing & Performance" \
  --iteration-count 100 \
  --reporters cli,htmlextra
```

---

## ðŸ“Š Test Coverage

### Complete Test Matrix

| Category | Tests | Assertions | Coverage |
|----------|-------|------------|----------|
| Infrastructure | 5 | 15 | Health, UI accessibility, Metrics |
| Session Management | 3 | 12 | Create, Persist, Attributes |
| API Gateway | 3 | 9 | Routes, Metrics, Distribution |
| Event-Driven | 2 | 5 | Order context, Event flow |
| Observability | 4 | 8 | JVM, App info, Config, Logs |
| Performance | 3 | 9 | Load, Throughput, Metrics |
| Cleanup | 2 | 4 | Logout, Final health |
| **Total** | **22** | **62** | **Full Stack** |

---

### Coverage Areas

âœ… **API Gateway**: Session management, Health checks, Actuator endpoints  
âœ… **Redis**: Session storage, Persistence, TTL, Memory  
âœ… **Prometheus**: Metrics collection, Exposition format  
âœ… **Jaeger**: Tracing UI accessibility  
âœ… **Grafana**: Dashboard UI accessibility  
âœ… **Spring Boot**: Actuator, Info, Environment, Loggers  
âœ… **Performance**: Load testing, Throughput, Response times  

---

## ðŸ“ˆ Interpreting Results

### Success Indicators

**âœ… All Tests Pass**:
```
Test Results: (62/62 passed)
Passed:  62
Failed:  0
Skipped: 0
```

**Console Output**:
```
âœ… API Gateway Health Check PASSED
âœ… SESSION CREATED
âœ… SESSION PERSISTED IN REDIS
âœ… GATEWAY ROUTES DISCOVERED
âœ… JVM METRICS CAPTURED
âœ… LOAD TEST SESSION CREATED
âœ… SESSION DESTROYED
ðŸŽ‰ ALL MICROSERVICE TESTS COMPLETED SUCCESSFULLY
```

---

### Performance Metrics

**Good Performance**:
```
API Gateway Health: < 500ms
Session Creation: < 1000ms
Session Retrieval: < 100ms
Metrics Endpoint: < 200ms
```

**Review Response Times**:
```javascript
// In Collection Runner results
View "Response Time" column
Sort by time to find slowest requests
```

---

### Common Test Failures

#### âŒ Health Check Fails

**Error**: `Service is DOWN`

**Solutions**:
```bash
# Check service status
docker-compose ps

# Restart unhealthy service
docker-compose restart api-gateway

# Check logs
docker-compose logs api-gateway
```

---

#### âŒ Session Not Created

**Error**: `Session ID is null`

**Solutions**:
```bash
# Verify Redis is running
docker exec redis-session redis-cli ping

# Check API Gateway can reach Redis
docker logs api-gateway | grep -i redis

# Restart services
docker-compose restart api-gateway redis
```

---

#### âŒ Metrics Not Available

**Error**: `Metrics endpoint returns 404`

**Solutions**:
```yaml
# Verify actuator configuration in application.yml
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
```

---

## ðŸ”§ Troubleshooting

### Pre-Test Checklist

Before running tests, verify:

```bash
# 1. All services running
docker-compose ps
# All should show "Up" and "healthy"

# 2. API Gateway accessible
curl http://localhost:8080/api/health

# 3. Redis accessible
docker exec redis-session redis-cli ping

# 4. Ports not blocked
netstat -ano | findstr :8080
netstat -ano | findstr :6379
```

---

### During Test Issues

#### Tests Timeout

```bash
# Increase timeout in Postman settings
Settings â†’ Request timeout â†’ 30000ms

# Or use Collection Runner â†’ Advanced â†’ Request timeout
```

---

#### Variables Not Set

```bash
# Check collection variables
Collection â†’ Variables tab

# Reset variables if needed
sessionId: (empty)
userId: (empty)
orderId: (empty)
```

---

#### Redis Connection Failed

```bash
# Test Redis connection
docker exec redis-session redis-cli KEYS "*"

# Check session storage
docker exec redis-session redis-cli KEYS "spring:session:*"

# Verify Redis memory
docker exec redis-session redis-cli INFO memory
```

---

### Post-Test Cleanup

```bash
# Clear old sessions (optional)
docker exec redis-session redis-cli FLUSHALL

# Restart services for clean state
docker-compose restart

# Check final health
curl http://localhost:8080/api/health
```

---

## ðŸŽ¯ Best Practices

### Before Testing

1. âœ… Ensure all services healthy
2. âœ… Clear old test data (optional)
3. âœ… Check network connectivity
4. âœ… Review collection variables

### During Testing

1. âœ… Run full suite first
2. âœ… Review each test result
3. âœ… Check console output
4. âœ… Monitor response times

### After Testing

1. âœ… Review test summary
2. âœ… Check for failed tests
3. âœ… Analyze performance metrics
4. âœ… Run cleanup requests

---

## ðŸ“š Additional Resources

### Related Documentation

- [Redis Session Quick Start](../REDIS_SESSION_QUICKSTART.md)
- [API Gateway README](../api-gateway/README.md)
- [Architecture Documentation](../Architecture.md)
- [Main README](../README.md)

### Monitoring Dashboards

- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)
- **Jaeger**: http://localhost:16686
- **Redis Commander**: http://localhost:8081

### Newman Documentation

- [Newman GitHub](https://github.com/postmanlabs/newman)
- [Newman CLI Options](https://learning.postman.com/docs/running-collections/using-newman-cli/command-line-integration-with-newman/)

---

## ðŸ¤ Contributing

Found issues or want to add more tests?

1. Fork the repository
2. Add new test scenarios
3. Update this guide
4. Submit pull request

---

## ðŸ“„ License

MIT License - See [LICENSE](../LICENSE) for details

---

<div align="center">

**ðŸŽ‰ Happy Testing!**

*Part of the Spring Boot Microservices with gRPC, Kafka & Redis project*

[GitHub Repository](https://github.com/HamdeeNaseng/Spring-Boot-gRPC) | [Report Issues](https://github.com/HamdeeNaseng/Spring-Boot-gRPC/issues)

</div>
